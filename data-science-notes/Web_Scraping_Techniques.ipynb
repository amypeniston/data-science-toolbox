{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Techniques\n",
    "\n",
    "*Various techniques for scraping data from websites.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking that a page exists\n",
    "\n",
    "HTTP response status codes come in five classes and the value of the first digit indicates the category of response:\n",
    "\n",
    "* 1xx - informational\n",
    "* 2xx - success\n",
    "* 3xx - redirection\n",
    "* 4xx - client error\n",
    "* 5xx - server error\n",
    "\n",
    "Thus, we can check for the status code being less than 400 and catch all of the various non-error responses.\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website exists (status code 200)\n",
      "Website does not exist\n"
     ]
    }
   ],
   "source": [
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "def check_website(request_response):\n",
    "    status_code = request_response.status_code\n",
    "    \n",
    "    if status_code < 400:\n",
    "        print(\"Website exists (status code {})\".format(status_code))\n",
    "    else:\n",
    "        print(\"Failed to retrieve website (status code {})\".format(status_code))\n",
    "\n",
    "urls = [\"https://www.nasa.gov/\", \"http://thiswebsitebroke.com/\"]\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        request_response = requests.get(url) # get response headers without downloading the entire page\n",
    "        check_website(request_response)\n",
    "    except ConnectionError:\n",
    "        print(\"Website does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the content of a website's robot.txt file\n",
    "\n",
    "The robot.txt file typically contains a list of all files that the owner intended to be scrapable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿# robots.txt for http://www.wikipedia.org/ and friends\n",
      "#\n",
      "# Please note: There are a lot of pages on this site, and there are\n",
      "# some misbehaved spiders out there that go _way_ too fast. If you're\n",
      "# irresponsible, your access to the site may be blocked.\n",
      "#\n",
      "\n",
      "# Observed spamming large amounts of https://en.wikipedia.org/?curid=NNNNNN\n",
      "# and ignoring 429 ratelimit responses, claims to respect robots:\n",
      "# http://mj12bot.com/\n",
      "User-agent: MJ12bot\n",
      "Disallow: /\n",
      "\n",
      "# advertising-related bots:\n",
      "User-agent: Mediapa\n"
     ]
    }
   ],
   "source": [
    "request_response = requests.get(\"https://en.wikipedia.org/robots.txt\")\n",
    "print(request_response.text[0:500]) # Preview a snippet of the robots.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting specific content from a website\n",
    "\n",
    "For example, extracting the number of datasets on data.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://catalog.data.gov/dataset\"\n",
    "\n",
    "result = requests.get(url)\n",
    "soup = BeautifulSoup(result.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'250,031 datasets found'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"div\", {\"class\": \"new-results\"}).text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding an address\n",
    "\n",
    "Utilize an address lookup website such as latlong.net to convert a street/city address into lat/lng coordinates. Manually enter an address, open Chrome dev tools and then submit the request. Inspect the request headers and form data that were sent for the request and copy required parts to structure the request programmatically. The trick is figuring out which parts of the headers and the form data are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "48.856613,2.352222\n"
     ]
    }
   ],
   "source": [
    "address = \"Paris, France\"\n",
    "\n",
    "url = \"https://www.latlong.net/_spm4.php\"\n",
    "headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36\",\n",
    "           \"x-requested-with\": \"XMLHttpRequest\"}\n",
    "data = {\"c1\": address, \"action\": \"gpcm\"}\n",
    "\n",
    "result = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "print(result)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe with names and urls for all datasets on FiveThirtyEight\n",
    "\n",
    "This requires parsing a large table of (very interesting) datasets and extracting appropriate URLs. Note that the slug-like data set name is not sufficient to construct a URL. See https://data.fivethirtyeight.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://data.fivethirtyeight.com/\"\n",
    "\n",
    "result = requests.get(url)\n",
    "soup = BeautifulSoup(result.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\", {\"id\": \"dataIndex\"})\n",
    "rows = table.find_all(\"tr\", {\"class\": \"article\"})\n",
    "\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    fields = row.find_all(\"td\", {\"class\": None})\n",
    "    for field in fields:\n",
    "        if field.find(\"a\", {\"class\": \"article-title\"}) != None:\n",
    "            url = field.find(\"a\", {\"class\": \"article-title\"})\n",
    "            data.append([field.text, url[\"href\"]])\n",
    "\n",
    "fivethirtyeight = pd.DataFrame(data, columns=[\"Dataset Name\", \"URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tracking Congress In The Age Of Trump</td>\n",
       "      <td>https://projects.fivethirtyeight.com/congress-trump-score/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-20 NBA Predictions</td>\n",
       "      <td>https://projects.fivethirtyeight.com/2020-nba-predictions/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019 MLB Predictions</td>\n",
       "      <td>https://projects.fivethirtyeight.com/2019-mlb-predictions/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Popular Is Donald Trump?</td>\n",
       "      <td>https://projects.fivethirtyeight.com/trump-approval-ratings/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Latest Polls</td>\n",
       "      <td>https://projects.fivethirtyeight.com/polls/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Dataset Name  \\\n",
       "0  Tracking Congress In The Age Of Trump   \n",
       "1  2019-20 NBA Predictions                 \n",
       "2  2019 MLB Predictions                    \n",
       "3  How Popular Is Donald Trump?            \n",
       "4  Latest Polls                            \n",
       "\n",
       "                                                            URL  \n",
       "0  https://projects.fivethirtyeight.com/congress-trump-score/    \n",
       "1  https://projects.fivethirtyeight.com/2020-nba-predictions/    \n",
       "2  https://projects.fivethirtyeight.com/2019-mlb-predictions/    \n",
       "3  https://projects.fivethirtyeight.com/trump-approval-ratings/  \n",
       "4  https://projects.fivethirtyeight.com/polls/                   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "fivethirtyeight.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all header tags from the front page of CNN\n",
    "\n",
    "You can pass an array of tags into the BeautifulSoup ```find()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.cbsnews.com/\"\n",
    "\n",
    "result = requests.get(url)\n",
    "soup = BeautifulSoup(result.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Stories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBSN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who stood out at the Democratic debate?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paul Rudd talks new Netflix series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pence, Pompeo to meet with Turkish leader</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0\n",
       "0  Top Stories                              \n",
       "1  CBSN                                     \n",
       "2  Who stood out at the Democratic debate?  \n",
       "3  Paul Rudd talks new Netflix series       \n",
       "4  Pence, Pompeo to meet with Turkish leader"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tags = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "headers = [header.text.strip() for header in h_tags if header.text.strip() != \"\"]\n",
    "    \n",
    "header_df = pd.DataFrame(headers)\n",
    "header_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse JSON from URL to get number of active users on US government sites\n",
    "\n",
    "Parsing json is as simple as calling ```.json()``` on the retrieved response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'realtime',\n",
       " 'sampling': {},\n",
       " 'query': {'metrics': ['rt:activeUsers'], 'max-results': 10000},\n",
       " 'meta': {'name': 'Active Users Right Now',\n",
       "  'description': 'Number of users currently visiting all sites.'},\n",
       " 'data': [{'active_visitors': '322685'}],\n",
       " 'totals': {},\n",
       " 'taken_at': '2019-10-16T20:26:31.628Z'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://analytics.usa.gov/data/live/realtime.json\"\n",
    "\n",
    "result = requests.get(url)\n",
    "\n",
    "result.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322685"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_users = int(result.json()[\"data\"][0][\"active_visitors\"])\n",
    "active_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the number of followers for a given twitter account\n",
    "\n",
    "This one is a bit tricky. Since twitter doesn't give us unique IDs to search for, we need to extract the followers text from the link element that contains the word \"followers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"GordonRamsay\"\n",
    "url = \"https://twitter.com/\" + username\n",
    "\n",
    "result = requests.get(url)\n",
    "soup = BeautifulSoup(result.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7256128"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_followers = soup.find(\"a\", href=re.compile(\"followers\"))[\"title\"]\n",
    "num_followers = re.findall(\"\\d\", num_followers)\n",
    "num_followers = int(\"\".join(num_followers))\n",
    "num_followers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
