{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Data Science Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Options\n",
    "\n",
    "**SQL** - well-established open standard, fast index access, high data normalization, costly, difficult to scale, schema changes require DDL\n",
    "\n",
    "**noSQL** - dynamic schema, linearly scalable, low storage cost, no data normalization/integrity constraints, less established, slower than SQL\n",
    "\n",
    "**ObjectStorage** - schema-less, linearly scalable, cheap\n",
    "\n",
    "In general, SQL is suitable for a small amounts of data requiring a stable schema. For larger amounts of data with high ingestion rates or frequent changes of schema, noSQL or ObjectStorage are appropriate. \n",
    "\n",
    "#### ApacheSpark\n",
    "\n",
    "ApacheSpark handles the parallelization of distributed data and processing across many compute (“worker”) nodes. While the underlying execution engine in ApacheSpark is implemented in Scala on top of a Java Virtual Machine (JVM), it has connectors for multiple programming languages including Python, R, Java and Scala. The various languages come with their own advantages and disadvantages, with Python and R falling on the easier-to-learn side of the spectrum, at the cost of performance.\n",
    "\n",
    "Multiple JVM instances can work in tandem on a single worker node, with the general rule of one JVM per CPU core. For example, a cluster with 100 nodes, 4 CPUs per node, 16 CPU cores per CPU and 4 hyperthreads per core could have 25,600 parallel threads running at the same time. Storage can either be connected via a fast network connection (off-node storage approach) or hard drives can be connected directly to worker nodes (Just a Bunch Of Disks aka JBOD approach). The second approach requires an additional software component called Hadoop Distributed File System (HDFS) to combine and present the disparate storage capacities into one virtual file system.\n",
    "\n",
    "A **Resilient Distributed Dataset (RDD)** is a distributed immutable collection that resides on the main memory of worker nodes. RDDs are lazy, meaning that data is not read from the underlying storage system unless it is needed. \n",
    "\n",
    "Distribute data across spark nodes: `rdd = sc.parallelize(range(100))`\n",
    "\n",
    "Trigger the execution of a Spark job which is divided into individual tasks that are executed in parallel across the cluster: `rdd.count()`\n",
    "\n",
    "View the first ten elements of the RDD: `rdd.take(10)`\n",
    "\n",
    "Copy the contents of the data to the local ApacheSpark driver JVM: `rdd.collect()` (be careful with doing this with large datasets as you can cause the driver JVM to crash due to exceeded memory capacity)\n",
    "\n",
    "#### Functional Programming (FP)\n",
    "\n",
    "The central concept of FP is Lambda Calculus, which enables computations to be expressed as anonymous functions. Scala is the most recent representative of FP, joining the likes of Haskell, while also supporting procedural and OOP. ApacheSpark parallelizes computations using Lambda Calculus.\n",
    "\n",
    "\n",
    "**Add 1 to each element of a list:**\n",
    "\n",
    "```\n",
    "rdd = sc.parallelize(range(100))\n",
    "rdd.map(lambda x: x+1).take(10)\n",
    "```\n",
    "\n",
    "**Sum elements of a list:**\n",
    "\n",
    "```\n",
    "sc.parallelize(range(1,101)).reduce(lambda a,b: a+b)\n",
    "```\n",
    "\n",
    "#### ApacheSparkSQL\n",
    "\n",
    "ApacheSparkSQL wraps the RDD with a DataFrame object,  abstracting the RDD API into a more familiar relational interface. This utility produces an abstract syntax tree, which is transformed into a logical query execution plan by the catalyst optimizer. This results in very high performance code that is more intuitive to write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Moments\n",
    "\n",
    "Statistical moments measure characteristics of distributions:\n",
    "\n",
    "**Average**\n",
    "\n",
    "The average is a measure of central tendency of a distribution. \n",
    "* Mean: The average of all values\n",
    "\n",
    "$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$\n",
    "\n",
    "* Median: The midpoint of a sorted distribution; more resilient to outliers\n",
    "\n",
    "\n",
    "**Standard Deviation**\n",
    "\n",
    "The standard deviation measures the spread of data around the mean. Data that is condensed around the mean will have a lower standard deviation.\n",
    "\n",
    "$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$\n",
    "\n",
    "**Skewness**\n",
    "\n",
    "Skewness represents the asymmetricity of the data or \"tail\" of the distribution, i.e. positive or negative skew. However, it does not capture the shape of the tail (whether thick and short or thin and long).\n",
    "\n",
    "$skewness = \\frac{1}{n}\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^3}{\\sigma^3}$\n",
    "\n",
    "**Kurtosis**\n",
    "\n",
    "Kurtosis measures the the outlier content of a distribution, i.e. the tail. The higher the kurtosis, the more outliers are presenet in the data and the longer the tail.\n",
    "\n",
    "$kurtosis = \\frac{1}{n}\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^4}{\\sigma^4}$\n",
    "\n",
    "**Covariance**\n",
    "\n",
    "Covariance is a measure of the interdependence of between columns of data.\n",
    "\n",
    "$cov(x,y) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})$\n",
    "\n",
    "**Correlation**\n",
    "\n",
    "Correlation is a measure of dependence that ranges from -1 to +1. Total positive/negative dependence results in a correlation of +1/-1, while data that shows no interaction has a correlation of 0. It is useful to utilize results in a correlation matrix when dealing with data in multiple columns.\n",
    "\n",
    "$corr(x,y) = \\frac{cov(x,y)}{\\sigma_x\\sigma_y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
