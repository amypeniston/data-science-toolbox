{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Data Science Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Options\n",
    "\n",
    "**SQL** - well-established open standard, fast index access, high data normalization, costly, difficult to scale, schema changes require DDL\n",
    "\n",
    "**noSQL** - dynamic schema, linearly scalable, low storage cost, no data normalization/integrity constraints, less established, slower than SQL\n",
    "\n",
    "**ObjectStorage** - schema-less, linearly scalable, cheap\n",
    "\n",
    "In general, SQL is suitable for a small amounts of data requiring a stable schema. For larger amounts of data with high ingestion rates or frequent changes of schema, noSQL or ObjectStorage are appropriate. \n",
    "\n",
    "#### ApacheSpark\n",
    "\n",
    "ApacheSpark handles the parallelization of distributed data and processing across many compute (“worker”) nodes. While the underlying execution engine in ApacheSpark is implemented in Scala on top of a Java Virtual Machine (JVM), it has connectors for multiple programming languages including Python, R, Java and Scala. The various languages come with their own advantages and disadvantages, with Python and R falling on the easier-to-learn side of the spectrum, at the cost of performance.\n",
    "\n",
    "Multiple JVM instances can work in tandem on a single worker node, with the general rule of one JVM per CPU core. For example, a cluster with 100 nodes, 4 CPUs per node, 16 CPU cores per CPU and 4 hyperthreads per core could have 25,600 parallel threads running at the same time. Storage can either be connected via a fast network connection (off-node storage approach) or hard drives can be connected directly to worker nodes (Just a Bunch Of Disks aka JBOD approach). The second approach requires an additional software component called Hadoop Distributed File System (HDFS) to combine and present the disparate storage capacities into one virtual file system.\n",
    "\n",
    "Central to Spark's abstraction is the **Resilient Distributed Dataset (RDD)**, a distributed immutable collection that resides on the main memory of worker nodes. RDDs are lazy, meaning that data is not read from the underlying storage system unless it is needed. RDDs are inherently fault tolerant and can spill over to disk if there is not enough RAM to store the data.\n",
    "\n",
    "Distribute data across spark nodes: `rdd = sc.parallelize(range(100))`\n",
    "\n",
    "Trigger the execution of a Spark job which is divided into individual tasks that are executed in parallel across the cluster: `rdd.count()`\n",
    "\n",
    "View the first ten elements of the RDD: `rdd.take(10)`\n",
    "\n",
    "Copy the contents of the data to the local ApacheSpark driver JVM: `rdd.collect()` (be careful with doing this with large datasets as you can cause the driver JVM to crash due to exceeded memory capacity)\n",
    "\n",
    "**Creating a RDD from External Data in Cloud Object Storage**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "df = spark.read.parquet(cos.url(file, bucket))\n",
    "df.createOrReplaceTempView('table')\n",
    "df.show()\n",
    "```\n",
    "\n",
    "#### Functional Programming (FP)\n",
    "\n",
    "The central concept of FP is Lambda Calculus, which enables computations to be expressed as anonymous functions. Scala is the most recent representative of FP, joining the likes of Haskell, while also supporting procedural and OOP. ApacheSpark parallelizes computations using Lambda Calculus.\n",
    "\n",
    "\n",
    "**Add 1 to each element of a list:**\n",
    "\n",
    "```\n",
    "rdd = sc.parallelize(range(100))\n",
    "rdd.map(lambda x: x+1).take(10)\n",
    "```\n",
    "\n",
    "**Sum elements of a list:**\n",
    "\n",
    "```\n",
    "sc.parallelize(range(1,101)).reduce(lambda a,b: a+b)\n",
    "```\n",
    "\n",
    "#### ApacheSparkSQL\n",
    "\n",
    "ApacheSparkSQL wraps the RDD with a DataFrame object,  abstracting the RDD API into a more familiar relational interface. This utility produces an abstract syntax tree, which is transformed into a logical query execution plan by the catalyst optimizer. This results in very high performance code that is more intuitive to write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Moments\n",
    "\n",
    "Statistical moments measure characteristics of distributions:\n",
    "\n",
    "**Average**\n",
    "\n",
    "The average is a measure of central tendency of a distribution. \n",
    "* Mean: The average of all values\n",
    "\n",
    "$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$\n",
    "\n",
    "* Median: The midpoint of a sorted distribution; more resilient to outliers\n",
    "\n",
    "```python\n",
    "rddX = sc.parallelize(random.sample(range(100),100)\n",
    "rddY = sc.parallelize(random.sample(range(100),100)\n",
    "\n",
    "meanX = rddX.sum()/float(rddX.count())\n",
    "```\n",
    "\n",
    "**Standard Deviation**\n",
    "\n",
    "The standard deviation measures the spread of data around the mean. Data that is condensed around the mean will have a lower standard deviation.\n",
    "\n",
    "$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$\n",
    "\n",
    "```python\n",
    "stdX = sqrt(rddX.map(lamba x: pow(x - meanX,2)).sum()/rddX.count())\n",
    "```\n",
    "\n",
    "**Skewness**\n",
    "\n",
    "Skewness represents the asymmetricity of the data or \"tail\" of the distribution, i.e. positive or negative skew. However, it does not capture the shape of the tail (whether thick and short or thin and long).\n",
    "\n",
    "$skewness = \\frac{1}{n}\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^3}{\\sigma^3}$\n",
    "\n",
    "```python\n",
    "skewness = (1 / rddX.count()) * rddX.map(lambda x: pow(x - meanX, 3)/pow(stdX, 3)).sum()\n",
    "```\n",
    "\n",
    "**Kurtosis**\n",
    "\n",
    "Kurtosis measures the the outlier content of a distribution, i.e. the tail. The higher the kurtosis, the more outliers are present in the data and the longer the tail.\n",
    "\n",
    "$kurtosis = \\frac{1}{n}\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^4}{\\sigma^4}$\n",
    "\n",
    "```python\n",
    "kurtosis = (1 / rddX.count()) * rddX.map(lambda x: pow(x - meanX, 4)/pow(stdX, 4)).sum()\n",
    "```\n",
    "\n",
    "**Covariance**\n",
    "\n",
    "Covariance is a measure of the interdependence of between columns of data.\n",
    "\n",
    "$cov(x,y) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})$\n",
    "\n",
    "\n",
    "```python\n",
    "rddXY = rddX.zip(rddY)\n",
    "\n",
    "covXY = rddXY.map(lambda x_y: (x_y[0] - meanX)(x_y[1] - meanY)).sum()/rddXY.count()                 \n",
    "```\n",
    "\n",
    "**Correlation**\n",
    "\n",
    "Correlation is a measure of dependence that ranges from -1 to +1. Total positive/negative dependence results in a correlation of +1/-1, while data that shows no interaction has a correlation of 0. It is useful to utilize results in a correlation matrix when dealing with data in multiple columns.\n",
    "\n",
    "$corr(x,y) = \\frac{cov(x,y)}{\\sigma_x\\sigma_y}$\n",
    "\n",
    "```python\n",
    "corrXY = covXY / (stdX * stdY)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "Sampling is critical when working with big data. This process involves selecting a subset of the entire dataset to feed into plotting libraries, which expect small inputs in the form of vectors or matrices. Even if these libraries were capable of handling massive quantities of data, we would run into memory and performance issues if we tried to utilize 100% of the information. Random sampling preserves sufficient properties in the original data to enable plotting.\n",
    "\n",
    "Matplotlib can be used with Spark to visualize data:\n",
    "\n",
    "* Box plots: distribution of data including mean, standard deviation, skew and outliers\n",
    "* Histograms: distribution of data within a particular dimension\n",
    "* Run charts: time series data visualization\n",
    "* Scatter plot: correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boxplot**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Extract desired data\n",
    "voltage = spark.sql(\"select voltage from table where voltage is not null\")\n",
    "\n",
    "# Create a Python list\n",
    "voltage_array = data.rdd.sample(False, 0.1).map(lambda row: row.voltage).collect()\n",
    "\n",
    "plt.boxplot(voltage_array)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run chart**\n",
    "\n",
    "```python\n",
    "data = spark.sql(\"select voltage, time from table where voltage is not null order by time asc\")\n",
    "\n",
    "data_sampled = data.rdd.sample(False, 0.1).map(lambda row: (row.voltage, row.time))\n",
    "voltage_array = data_sampled.rdd.map(lambda x: x[0]).collect()\n",
    "time_array = data_sampled.rdd.map(lambda x: x[1]).collect()\n",
    "\n",
    "plt.plot(time_array, voltage_array)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"voltage\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D Plotting**\n",
    "\n",
    "```python\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "select hardness,temperature,flowrate from washing\n",
    "    where hardness is not null and \n",
    "    temperature is not null and \n",
    "    flowrate is not null\n",
    "\"\"\")\n",
    "\n",
    "result_rdd = result_df.rdd.sample(False,0.1).map(lambda row : (row.hardness,row.temperature,row.flowrate))\n",
    "result_array_hardness = result_rdd.map(lambda hardness_temperature_flowrate: hardness_temperature_flowrate[0]).collect()\n",
    "result_array_temperature = result_rdd.map(lambda hardness_temperature_flowrate: hardness_temperature_flowrate[1]).collect()\n",
    "result_array_flowrate = result_rdd.map(lambda hardness_temperature_flowrate: hardness_temperature_flowrate[2]).collect()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(result_array_hardness,result_array_temperature,result_array_flowrate, c='b', marker='o')\n",
    "\n",
    "ax.set_xlabel('Hardness')\n",
    "ax.set_ylabel('Temperature')\n",
    "ax.set_zlabel('Flowrate')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histogram**\n",
    "\n",
    "```python\n",
    "plt.hist(result_array_hardness)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "Dimensionality reduction transforms an n-dimensional euclidian vector space into a new dataset in a lower dimension where the distance between points are preserved. After this projection process has been applied, we are left with artificial dimensions, called principal components, ordered by the amount of variation that each PC explains in the original dataset. **Using PCA to reduce dimensionality to 3 enables us to utilze 3D plotting libraries**. \n",
    "\n",
    "While powerful, PCA has its tradeoffs. One downfall of PCA is that principal components are usually less interpretable than the original features that they represent. In addition, the algorithm is lossy. However, the amount of information that is lost during dimensionality can be calculated and managed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define vector transformation helper class which takes all input features (result.columns) and creates an additional column containing all input features in a single column format wrapped in \"DenseVector\" objects\n",
    "assembler = VectorAssembler(inputCols=result.columns, outputCol = \"features\")\n",
    "\n",
    "# Transform the data\n",
    "features = assembler.transform(result)\n",
    "\n",
    "# Prepare algorithm and then apply PCA, desired number of features = 3\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(features)\n",
    "\n",
    "# Transform the data\n",
    "result_pca = model.transform(features).select(\"pcaFeatures\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
