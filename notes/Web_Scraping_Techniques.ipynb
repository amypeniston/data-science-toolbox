{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Techniques\n",
    "\n",
    "*Various techniques for scraping data from websites.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking that a page exists\n",
    "\n",
    "HTTP response status codes come in five classes and the value of the first digit indicates the category of response:\n",
    "\n",
    "* 1xx - informational\n",
    "* 2xx - success\n",
    "* 3xx - redirection\n",
    "* 4xx - client error\n",
    "* 5xx - server error\n",
    "\n",
    "Thus, we can check for the status code being less than 400 and catch all of the various non-error responses.\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website exists (status code 200)\n",
      "Website does not exist\n"
     ]
    }
   ],
   "source": [
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "def check_website(request_response):\n",
    "    status_code = request_response.status_code\n",
    "    \n",
    "    if status_code < 400:\n",
    "        print(\"Website exists (status code {})\".format(status_code))\n",
    "    else:\n",
    "        print(\"Failed to retrieve website (status code {})\".format(status_code))\n",
    "\n",
    "urls = [\"https://www.nasa.gov/\", \"http://thiswebsitebroke.com/\"]\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        request_response = requests.get(url) # get response headers without downloading the entire page\n",
    "        check_website(request_response)\n",
    "    except ConnectionError:\n",
    "        print(\"Website does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the content of a website's robot.txt file\n",
    "\n",
    "The robot.txt file typically contains a list of all files that the owner intended to be scrapable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿# robots.txt for http://www.wikipedia.org/ and friends\n",
      "#\n",
      "# Please note: There are a lot of pages on this site, and there are\n",
      "# some misbehaved spiders out there that go _way_ too fast. If you're\n",
      "# irresponsible, your access to the site may be blocked.\n",
      "#\n",
      "\n",
      "# Observed spamming large amounts of https://en.wikipedia.org/?curid=NNNNNN\n",
      "# and ignoring 429 ratelimit responses, claims to respect robots:\n",
      "# http://mj12bot.com/\n",
      "User-agent: MJ12bot\n",
      "Disallow: /\n",
      "\n",
      "# advertising-related bots:\n",
      "User-agent: Mediapa\n"
     ]
    }
   ],
   "source": [
    "request_response = requests.get(\"https://en.wikipedia.org/robots.txt\")\n",
    "print(request_response.text[0:500]) # Preview a snippet of the robots.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting specific content from a website\n",
    "\n",
    "For example, extracting the number of datasets on data.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://catalog.data.gov/dataset\"\n",
    "\n",
    "result = requests.get(url)\n",
    "soup = BeautifulSoup(result.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'250,032 datasets found'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"div\", {\"class\": \"new-results\"}).text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding an address\n",
    "\n",
    "Utilize an address lookup website such as latlong.net to convert a street/city address into lat/lng coordinates. Manually enter an address, open Chrome dev tools and then submit the request. Inspect the request headers and form data that were sent for the request and copy required parts to structure the request programmatically. The trick is figuring out which parts of the headers and the form data are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "48.856613,2.352222\n"
     ]
    }
   ],
   "source": [
    "address = \"Paris, France\"\n",
    "\n",
    "url = \"https://www.latlong.net/_spm4.php\"\n",
    "headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36\",\n",
    "           \"x-requested-with\": \"XMLHttpRequest\"}\n",
    "data = {\"c1\": address, \"action\": \"gpcm\"}\n",
    "\n",
    "result = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "print(result)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the name of the newest dataset on Kaggle\n",
    "\n",
    "*In progress*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.kaggle.com/requests/SearchDatasetsRequest\"\n",
    "headers = {\"origin\": \"https://www.kaggle.com\",\n",
    "\"referer\": \"https://www.kaggle.com/datasets?sort=published\",\n",
    "\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36\"}\n",
    "\n",
    "result = requests.post(url, headers=headers)\n",
    "soup = BeautifulSoup(result.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [400]>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", {\"class\": \"datasets\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
