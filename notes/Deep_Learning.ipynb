{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied AI: Deep Learning\n",
    "\n",
    "Key components of cloud architecture for applied AI:\n",
    "* Object Storage - for capacity, redundancy, automated backup and I/O performance\n",
    "* Real-Time Data Stream - for real-time ingestion and model applications\n",
    "* Scaling - how to scale models on GPU and CPU clusters\n",
    "* Jupyter Notebooks - to create models\n",
    "* Deep Learning Framework - high or low level options to implement/test neural networks (ex: Keras)\n",
    "* Open Neural Network Exchange Formats - to facilitate import/export between frameworks (ex: ONNX)\n",
    "* Execution Environment - for large scale parallel execution (ex: DeepLearning4J or Apache SystemML on top of Apache Spark)\n",
    "\n",
    "Popular deep learning frameworks include Keras, CNTK and Theano (depricated). Keras, the de facto choice, uses TensorFlow as the execution engine and can export models to be ingested by other frameworks such as DeepLearning4J and Apache System ML via open standard exchange formats such as ONNX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Given an input vector and associated weight/parameter vectors, a neural network attempts to minimize the error between estimated $\\hat{y}$ and actual output $y$, also known as the cost function $J$. It does this by optimizing the weights vectors for all training data.\n",
    "\n",
    "Although grid search (brute force) and Monte Carlo can be used to determine the correct weights, they are far too computationally expensive to be used in any practical applications. Instead, **gradient descent** is used to iteratively refine weights in the \"downhill\" direction along the hypersurface of the cost function. That is, parameters $\\theta$ are updated for each timestep $t$ such that: $\\theta_{t+1}= \\theta_t - \\eta\\Delta_\\theta J(\\theta_t, X, Y)$, where $\\eta$ is a chosen learning rate, $\\Delta_\\theta J(\\theta_t, X, Y)$ is the derivative of the cost function, $X$ is the input vector and $Y$ is the output vector. Other methods seek to improve upon vanilla gradient descent:\n",
    "\n",
    "* **Stochastic Gradient Descent**: by taking the gradient for each training example\n",
    "  ** More efficient, can be used for online data/streaming\n",
    "  ** Requires tuning of learning rate to ensure convergence; otherwise, tends to bounce around\n",
    "* **Mini-Batch Gradient Descent**: by taking the gradient for localized batches of points (typically between 50 and 256 values)\n",
    "  ** Both efficient and stable convergence\n",
    "  ** Takes advantage of speedy matrix derivative calculations\n",
    "\n",
    "Still, additional challenges remain, such as tuning/adapting the learning rate over time and for specific parameter upates. Other important algorithms that implement different updater strategies include:\n",
    "\n",
    "* **Momentum**: accelerates SGD in the correct direction and smooths oscillations by carrying forward a \"momentum term\" from the previous timestep\n",
    "* **Nesterov Accelerated Gradient**: gives Momentum a forward-looking intelligence, i.e. \"smart ball\"\n",
    "* **Adagrad**: adapts learning rate to the parameters based on gradients that have been previously calculated, allowing us to deal with sparse data\n",
    "* **Adadelta**: a version of Adagrad that attempts to more intelligently update learning rate\n",
    "* **RMSprop**: similar to Adadelta\n",
    "* **Adam**: RMSprop plus Momentum\n",
    "* **AdaMax**: update to Adam\n",
    "* **Nadam**: Adam plus Nesterov Accelerated Gradient\n",
    "\n",
    "These algorithms produce varying results in terms of convergence on optima, particularly around saddle points.\n",
    "\n",
    "Selecting the correct activation function for a given problem requires knowledge of the problems linearity. Non-linear functions can be approximated only using a non-linear activation function, such as sigmoid or tanh (similar to sigmoid but covering negative values). Multi-class classification applications also utilize softmax, which produces an ensemble of values that sum to one. Relu (Rectified Linear Unit) is the most widely used activation function due to its simplicity, however it can cause dead neurons; in these cases leaky Relu, which extends into the negative range, can be used instead. Often it is best to start with Relu for input and hidden layers and adjust the output layer based on the task at hand: regressor - linear output unit, classifier - softmax or sigmoid.\n",
    "\n",
    "Due to neural networks inherent flexibility in approximating any function, they tend to fit datasets (and noise) extremely well. To prevent overfitting, always compare training and validation results; the former should never be much higher than the latter. You can also use regularization, to penalize higher weights, and early stopping, to halt training at a certain threshold. Lastly, you can use Drop Out to randomly deactivate neurons at each epoch (training iteration), forcing the network to generalize.\n",
    "\n",
    "Why neural networks? Linear machine learning models are limited to linear functions; on the other hand, neural networks can be used when data isn't linearly separable. \n",
    "\n",
    "#### Deep Feedforward Neural Networks\n",
    "\n",
    "The simplest type of neural network is called a **perceptron**, which consists of a linear combination of an input vector and a weights vector, passed into a step activation function. This system acts as a binary linear classifier and is used to approximate some function. Deep feedforward neural networks consist of mutilayer perceptrons, with information flowing in the forward direction, through a hidden layer, in order to calculate the function output. \n",
    "\n",
    "Deep feedforward neural networks can represent any mathematical function (the Universal Function Approximation Theory). However, even if you can represent any mathetmatical function, having a single hidden layer is not viable for training the network.\n",
    "\n",
    "#### Convolutional Neural Networks\n",
    "\n",
    "Convolutional neural networks are favored for image classification due to their lower computational cost and ability to capture pixel dependencies throughout the image. Deep feedforward neural networks can represent any mathematical function.\n",
    "\n",
    "#### Recurrent Neural Networks\n",
    "\n",
    "While deep feedforward networks are effective at learning functions, they do not work well with sequences or time series data - enter RNNs. In these networks, feedback connections between neurons pass back temporal information, giving the system a form of memory.\n",
    "\n",
    "#### Long Short Term Networks\n",
    "\n",
    "LSTMs map an input vector to an output vector using weights and an activation function along with additional components, including an input gate, an output gate and a forget gate. Data flows through a central node called a cel state, which is the memory of the neuron. \n",
    "\n",
    "The input vector is used not only as input to the neuron but also input to the input gate. This gate has its own weights vector, which enables it to modulate the influx of information into the cell state. Likewise, the output gate, which controls the output to downstream neurons, takes the input vector and the actual cell state and applies a weights vector. FInally, the neuron needs a way to forget the cell state, hence the addition of a forget gate. This gate is controlled by the input vector and the current cell state to control how much of the prior state is preserved.\n",
    "\n",
    "#### Autoencoders\n",
    "\n",
    "Autoencoders map an input vector to itself via a bottlenecking architecture. In other words, it attempts to reconstruct a dataset by mimicing the identity function. Since intermediary layers have fewer neurons that outer layers, data must be compressed, forcing the network to learn efficient compression. Autoencoders are outperforming longstanding dimensionality reduction techniques including PCA (linear) and t-distributed Stochastic Neighbor Embedding aka t-SNE (non-linear).\n",
    "\n",
    "One application of autoencoders is anomaly detection. Since the network must learn how to reconstruct the training data, if it fails to do so on subsequent data, it is likely that that data is anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "TensorFlow, originally created by Google Brain, is an open-source symbolic math library for tasks involving heavy numerical computations. While not limited to one specific field, its main application is machine learning, particularly deep neural networks. TensorFlow enables algorithms to run at scale across a cluster backed by CPUs, GPUs, TPUs and mobile devices.\n",
    "\n",
    "In TensorFlow, every numerical computation is expressed as a graph, with nodes representing computations and links representing the flow of multidimensional arrays (tensors). **Placeholders** enable us to add data (ex: training data) to the computational graph during execution time, after the graph is constructed. **Variables** represent tensors whose values can be changed during training (ex: weights and biases). In TensorFlow, no computation takes place until after the computational graph is constructed and a session is instantiated. This deploys the execution graph onto an execution context (ex: CPU or GPU). \n",
    "\n",
    "In a healthy neural network, accuracy and loss should be inversely related during training. Convergence to the local optimum can be visualized using TensorBoard and adjustments to the learning rate might be required to dampen oscillations. In assessing the weights histogram, extreme values indicate oversaturation, while a uniform distribution indicates insufficient parameter updates. Weights centered very close to zero mean that the gradients are very small.\n",
    "\n",
    "A nice secondary output of TensorFlow is its so-called automatic differentiation. Since every operator registers the first derivative of its operation, TF can compute the derivative of any complex function by applying the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Intro: MNIST Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Preview a digit and a label\n",
    "batch_xs, batch_ys = mnist.train.next_batch(1)\n",
    "X = batch_xs\n",
    "X = X.reshape([28, 28])\n",
    "plt.gray()\n",
    "print(batch_ys)\n",
    "plt.imshow(X)\n",
    "\n",
    "# Set up variables/placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None,784]) # Training data\n",
    "W = tf.Variable(tf.zeros([784,10])) # Weights\n",
    "b = tf.Variable(tf.zeros([10])) # Biases\n",
    "y_ = tf.placeholder(tf.float32, [None, 10]) # Training labels\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b) # Create model\n",
    "\n",
    "# Cross entropy cost function: the sum of predicted values multiplied by log(actual values)\n",
    "cross_entropy = tf.reduce_mean( -tf.reduce_sum( y_ * tf.log(y), reduction_indices=[1] ) )\n",
    "\n",
    "# Gradient descent with learning rate = 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Create a session, necessary to deploy a computation graph on a specific context\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize all variables\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Gradient descent loop\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x:batch_xs, y_:batch_ys})\n",
    "    \n",
    "# Create boolean vector of equality between predictions and actuals\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "# Determine number of correctly predicted values\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Execute via session to calcuate accuracy on test data\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "Keras is a deep learning framework written in Python that is known for its strong user base and support documentation. Test data sets are widely available as are pre-built models. Keras can be hooked into a variety of backends including TensorFlow, CNTK and Theano.\n",
    "\n",
    "There are two types of models in Keras: **Sequential** and **Model (non-sequential)**. To create a sequential model consisting of stacked layers:\n",
    "\n",
    "* Instatiate a Sequential model\n",
    "* Add layers, one by one\n",
    "* Compile the model using a loss function (ex: mean squared error) and optimizer (ex: SGD)\n",
    "* Fit the model to the training data\n",
    "* Evaluate the model\n",
    "* Apply the model to new data to generate predictions\n",
    "\n",
    "Non-sequential models following a functional programming API.\n",
    "\n",
    "Keras models can be saved either as a complete model (architecture, weights and training configuration - HDF5) or as individual components (JSON or YAML).\n",
    "\n",
    "#### Example Feedforward Network\n",
    "\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation=\"relu\", input_shape(784,)) # 512 output\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation=\"relu\") # 512 output\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation=\"softmax\")) # 10 output\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)          \n",
    "\n",
    "print(\"Loss: {}, Accuracy: {}\".format(score[0], score[1]))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
