{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning & Signal Processing\n",
    "\n",
    "#### Linear Algebra Terminology Review\n",
    "\n",
    "* Scalar: numerical values ex: 1, 5, 42, pi\n",
    "* Vector: a one-dimensional array (m rows x 1 col)\n",
    "* Matrix: a two-dimensional array (m rows x n cols)\n",
    "* Tensor: any multi-dimensional array of numbers, for example: rank 0 (scalar), rank 1 (vector), rank 2 (matrix), rank 3 (3D matrix)\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "More broadly, tensors are a collection of vectors and covectors that are combined using the tensor product. Tensors feature heavily in the field of quantum computing. When two quantum systems are entangled together, their state vectors have been combined using the tensor product (circle with x). entanglement\n",
    "\n",
    "#### Sparse Vectors\n",
    "\n",
    "Sparse vectors contain predominantly zero values.\n",
    "\n",
    "Ex: (12, [3], [1.0]) = 12 elements with a 1.0 in position 3\n",
    "\n",
    "#### Spark ML\n",
    "\n",
    "**StringIndexer** = a class that transforms a string class label into a numerical class index\n",
    "\n",
    "**OneHotEncoder** = a class that transforms a column containing multiple values into a one-hot encoded vector with multiple binary elements, one for each original value\n",
    "\n",
    "**VectorAssembler** = a class that transforms a set of columns into a single DenseVector representation.\n",
    "\n",
    "**Pipelines** speed up ML development and enable us to express an end-to-end workflow within a single framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Example\n",
    "\n",
    "```python\n",
    "# Retrieve data from repo\n",
    "!git clone url_to_data\n",
    "\n",
    "# Confirm data download\n",
    "!ls dataset_name\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('x', IntegerType(), True),\n",
    "    StructField('x', IntegerType(), True),\n",
    "    StructField('x', IntegerType(), True)])\n",
    "\n",
    "file_list = os.listdir(\"dataset_name\")\n",
    "file_list_filtered = [f for f in file_list if \"_\" in f]\n",
    "\n",
    "df = None\n",
    "\n",
    "# Iterate through files, appending file data to end of dataframe\n",
    "for category in file_list_filtered:\n",
    "    data_files = os.listdir(\"dataset_name/\", category)\n",
    "    \n",
    "    for data_file in data_files:\n",
    "        print(data_file)\n",
    "        temp_df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \" \").csv(\"dataset_name/\" + category + '/' + data_file, schema=schema)\n",
    "        temp_df = temp_df.withColumn(\"class\", lit(category))\n",
    "        temp_df = temp_df.withColumn(\"source\", lit(data_file))\n",
    "        \n",
    "        if df is None:\n",
    "            df = temp_df\n",
    "        else:\n",
    "            df = df.union(temp_df)\n",
    "\n",
    "# Assign numerical value to each class\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "# One hot encode a sparse vector representing the numerical class index\n",
    "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVector\")\n",
    "encoded = encoder.transform(indexed)\n",
    "\n",
    "# Creates an vector object representing input columns to be passed into an ml algorithm\n",
    "vectorAssembler = VectorAssembler(inputCols=['x','y','z'], outputCol=\"features\")\n",
    "features_vectorized = vectorAssembler.transform(encoded)\n",
    "\n",
    "# Normalize features\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "normalized = normalizer.transform(features_vectorized)\n",
    "\n",
    "# Create a pipeline with the desired data processing stages\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\n",
    "model = pipeline.fit(df)\n",
    "prediction = model.transform(df)\n",
    "\n",
    "# Visualize the transformations\n",
    "prediction.show()\n",
    "\n",
    "# Drop unnecessary columns, leaving only the processed features column and the vectorized category column\n",
    "df_train = prediction.drop('x').drop('y').drop('z').drop(\"class\").drop(\"source\").drop(\"features\").drop(\"classIndex\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System ML\n",
    "\n",
    "System ML enables algorithms to be reused across data-parallel frameworks such as Hadoop and Spark, streamlining the deployment process in varying environments. It provides an API called MLContext that allows the user to register RDDs and Dataframes that were previously created through Spark SQL or other libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning with Spark ML\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "First, create a Vector Assembler and Normalizer. Then create a Linear Regression model. Finally, combine stages into a Pipeline.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer, lr])\n",
    "model = pipeline.fit(df)\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# r2 value\n",
    "print(model.stages[2].summary.r2)\n",
    "```\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "Logistic regression is simply linear regression that has been passed into a sigmoid function. It is a supervised machine learning algorithm used to predict discrete categorical values.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.regression import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities & Naive Bayes\n",
    "\n",
    "* Marginal probability - independent of any other event\n",
    "* Joint probability - probability of events occuring together\n",
    "* Conditional probability - probability of an event given that another event has occurred\n",
    "\n",
    "**Bayes Rule Derivation**\n",
    "\n",
    "* Sum Rule: $P(x) = \\sum_{y}P(x,y)$\n",
    "* Product Rule: $P(x,y) = P(y|x)p(x)$\n",
    "\n",
    "Rearranging the product rule, we can derive the Bayes rule:\n",
    "\n",
    "$P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$\n",
    "\n",
    "This enables us to describe the probability of an event occuring based on prior knowledge of other events.\n",
    "\n",
    "**Gaussian Distribution**\n",
    "\n",
    "The Gaussian (or Normal) distribution is a very common continuous distribution that occurs naturally in nature. Because it is a valid probability density function, the area under the curve always sums to one. The Guassian is often used in machine learning because it is a byproduct of sampling any random distribution with finite variance. However, Bayes can also utilize different distributions, including Binomial and Multinomial.\n",
    "\n",
    "$N(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n",
    "\n",
    "Bayesian inference is the process of adjusting the probability of a hypothesis as new evidence becomes available. This involves:\n",
    "\n",
    "* Obtaining a prior hypothesis (distribution) $P(H)$\n",
    "* Collecting of new data $E$ with a marginal likelihood $P(E)$\n",
    "* Calculating the likelihood, i.e. how compatible the new data is to our prior knowledge of existing data $P(E|H)$\n",
    "* Obtaining a posterior, i.e. the probability of our hypothesis $P(H|E) = \\frac{P(E|H)*P(H)}{P(E)}$\n",
    "\n",
    "The likelihood is calculated by plugging the new data into a guassian equation, which is defined by the $\\mu$ and $\\sigma$ of the original data.\n",
    "\n",
    "The goal is to maximize the posterior distribution, i.e. select the $H$ which maximizes $\\frac{P(E|H)*P(H)}{P(E)}$. Notice that the denominator can be ignored, leaving only the numerator. This is called the maximum a posteriori aka MAP.\n",
    "\n",
    "Naive Bayes is \"naive\" because it assumes that, when $x$ is a vector with multiple features, that all features are conditionally independent. This enables us to make a simplification in our calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "SVM is a binary linear classifier that finds the best hyperplane of separation between point clouds. Often, it is necessary to transform data into another feature space using kernels in order to identify a separation boundary. SVM can also be used with multiple classes by iterating through classes one by one (\"one versus all approach\").\n",
    "\n",
    "```python\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Evaluation Measures\n",
    "\n",
    "* Precision: $\\frac{t_p}{t_p + f_p}$ - number of correct instances divided by total number of selections made\n",
    "* Recall: $\\frac{t_p}{t_p + f_n}$ - number of correct instances divided by total number of instances that should have been identified\n",
    "* F1 Score: $\\frac{2 * precision * recall}{precision + recall}$ - the harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "\n",
    "Decision Trees, while poor performers on their own, become quite powerful and are less prone to overfitting when ensembled. Bootstrap aggregation or bagging is the process of splitting a dataset up into smaller subsets and training a Decision Tree model on each subset to create a \"Random Forest\". Results are then aggregated. Note that boostrapping is a parallel process.\n",
    "\n",
    "An alternative technique called Boosting involves sequentially training models using the residuals (error) of the previous model as input. This process, which creates \"Gradient Boosted Trees\", is more computationally expensive but can be improved using XGBoost.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create vector of input features\n",
    "vectorAssembler = VectorAssembler(inputCols=['X', 'Y', 'Z'], outputCol=\"features\")\n",
    "\n",
    "# Create model, identifying input/output cols\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Feed into pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, classifier])\n",
    "model = pipeline.fit(df)\n",
    "prediction = model.transform(df)\n",
    "prediction.show()\n",
    "\n",
    "# Evaluate\n",
    "binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"CLASS\")\n",
    "binEval.evaluate(prediction) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossvalidation, Gridsearch & Testing\n",
    "\n",
    "Using a validation set is important to prevent overfitting. However, this means that a section of data must be reserved, which is not ideal. Instead, crossvalidation is used to iteratively split data into training and validation folds. Model results are then averaged.\n",
    "\n",
    "Gridsearch optimizes models by iterating over the multi-dimensional hyperparameter space. Two important things to keep in mind with Gridsearch: 1) tuning parameters requires an additional test set to assess overfitting and 2) computational complexity increases exponentially with each hyperparameter.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(normalizer.p, [1.0,2.0,10.0]).addGrid(gbt.maxBins, [2,4,8,16]).addGrid(gbt.maxDepth, [2,4,8,16]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=MulticlassClassificationEvaluator(), numFolds=4)\n",
    "cvModel = crossval.fit(df_train)\n",
    "prediction = cvModel.transform(df_test)\n",
    "binEval.evaluate(prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "\n",
    "Regularization is used to prevent overfitting by penalizing extra features that are added to a model. One type of regularized regression (L1 or L2) called lasso regression that minimizes $SSE + \\lambda|\\beta|$, or the sum of squared errors plus the sum of each parameter $\\lambda$ multiplied by their respective coefficients of regression $\\beta$. Thus an equilibrium is reached between the goodness of fit and the number of features that are used in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Machine Learning\n",
    "\n",
    "Unsupervised ML requires that we understand distance between points. Commonly, this involves Euclidian Distance, which is an extension of the Pythagorean formula in vector spaces of any dimension. Another example of a distance measure is manhattan distance.\n",
    "\n",
    "Euclidian Distance in n dimensions: $d(p,q) = d(q,p) = \\sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2 + ... + (q_n - p_n)^2}$\n",
    "\n",
    "#### k-Means Clustering\n",
    "\n",
    "* Initialize k cluster centroids in n-dimensional hyerspace\n",
    "* Calculate distances between each point and each cluster centroid\n",
    "* Assign points to nearest cluster\n",
    "* Re-calculate cluster centroids based on the mean of points in each cluster space\n",
    "* Repeat until distance between points and cluster centroids converges on a minimum\n",
    "\n",
    "k-Means is a very naive algorithm that has several shortfalls:\n",
    "\n",
    "* It requires a preset number of clusters\n",
    "* It incorporates the entire dataset, thus is impacted by outliers\n",
    "* It might incorrectly identify clusters due to initial centroid placement\n",
    "\n",
    "```python\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(5).setSeed(1)\n",
    "pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Compute within set of sum squared errors\n",
    "wssse = model.stages[1].computeCost(vectorAssembler.transform(df))\n",
    "```\n",
    " \n",
    "#### Hierarchical Clustering\n",
    "\n",
    "Unlike k-Means, Hierarchical Clustering algorithms do not require the number of clusters to be pre-specified. They also offer the ability to learn non-spherical boundaries. In this technique, points are grouped with nearby neighbors in discrete time-steps creating a history of clustering that can be reviewed by the end-user. This means that the number of clusters can be chosen after the fact.\n",
    "\n",
    "#### Density-Based Clustering aka DBSCAN\n",
    "\n",
    "DBSCAN offers improvement clustering capabilities and outlier identification. The model is initialized with two parameters: epsilon, or maximum radius, and number of points, or the minimum number of points in the epsilon neighborhood that are required to define a cluster.\n",
    "\n",
    "* Randomly select a point and classify it as either an outlier, a border or a core point.\n",
    "* If outlier, randomly select another point.\n",
    "* If border or core, jump to all reachable points within the maximum radius and add them to the cluster.\n",
    "\n",
    "DBSCAN is the preferred clustering technique due to its resilience to noise.\n",
    "\n",
    "#### Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a broad term for techniques such as feature selection, which is often driven by domain knowledge, and feature reduction, which is achieved by constructing a new, smaller feature set that retains most of the relevant information. **Principal Component Analysis (PCA)** is a widely used mechanism for feature reduction.\n",
    "\n",
    "The idea behind PCA is that we want to find the direction in the dataset that preserves the most amount of variance. The data can then be projected onto this direction, which becomes the new coordinate system. For example, in 2D space, data points are projected onto the line that defines the direction that maximally explains variance, transforming the data from 2D into 1D. In 3D space, you would repeat this process twice: first, finding the vector that explains the most variance and then finding another vector, perpendicular to the first, that explains the most of the remaining variance. You could then project either once (to 2D) or twice (to 1D) to reduce the dimensionality of the data.\n",
    "\n",
    "PCA Process:\n",
    "\n",
    "* Center the data by subtracting the mean\n",
    "* Compute the covariance matrix sigma\n",
    "* Find the eigenvectors and eigenvalues of sigma and sort by decreasing eigenvalue\n",
    "    * The eigenvectors become the principal components, or the lines that explain the most of the variance\n",
    "    * The eigenvalues represent the amount of explained variance for each eigenvector\n",
    "* Select desired number of new dimensions and project the data by multiplying the transpose of each observation by each of the eigenvectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
