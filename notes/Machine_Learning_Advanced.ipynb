{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning & Signal Processing\n",
    "\n",
    "#### Linear Algebra Terminology Review\n",
    "\n",
    "* Scalar: numerical values ex: 1, 5, 42, pi\n",
    "* Vector: a one-dimensional array (m rows x 1 col)\n",
    "* Matrix: a two-dimensional array (m rows x n cols)\n",
    "* Tensor: any multi-dimensional array of numbers, for example: rank 0 (scalar), rank 1 (vector), rank 2 (matrix), rank 3 (3D matrix)\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "More broadly, tensors are a collection of vectors and covectors that are combined using the tensor product. Tensors feature heavily in the field of quantum computing. When two quantum systems are entangled together, their state vectors have been combined using the tensor product (circle with x). entanglement\n",
    "\n",
    "#### Sparse Vectors\n",
    "\n",
    "Sparse vectors contain predominantly zero values.\n",
    "\n",
    "Ex: (12, [3], [1.0]) = 12 elements with a 1.0 in position 3\n",
    "\n",
    "#### Spark ML\n",
    "\n",
    "**StringIndexer** = a class that transforms a string class label into a numerical class index\n",
    "\n",
    "**OneHotEncoder** = a class that transforms a column containing multiple values into a one-hot encoded vector with multiple binary elements, one for each original value\n",
    "\n",
    "**VectorAssembler** = a class that transforms a set of columns into a single DenseVector representation.\n",
    "\n",
    "**Pipelines** speed up ML development and enable us to express an end-to-end workflow within a single framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Example\n",
    "\n",
    "```python\n",
    "# Retrieve data from repo\n",
    "!git clone url_to_data\n",
    "\n",
    "# Confirm data download\n",
    "!ls dataset_name\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('x', IntegerType(), True),\n",
    "    StructField('x', IntegerType(), True),\n",
    "    StructField('x', IntegerType(), True)])\n",
    "\n",
    "file_list = os.listdir(\"dataset_name\")\n",
    "file_list_filtered = [f for f in file_list if \"_\" in f]\n",
    "\n",
    "df = None\n",
    "\n",
    "# Iterate through files, appending file data to end of dataframe\n",
    "for category in file_list_filtered:\n",
    "    data_files = os.listdir(\"dataset_name/\", category)\n",
    "    \n",
    "    for data_file in data_files:\n",
    "        print(data_file)\n",
    "        temp_df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \" \").csv(\"dataset_name/\" + category + '/' + data_file, schema=schema)\n",
    "        temp_df = temp_df.withColumn(\"class\", lit(category))\n",
    "        temp_df = temp_df.withColumn(\"source\", lit(data_file))\n",
    "        \n",
    "        if df is None:\n",
    "            df = temp_df\n",
    "        else:\n",
    "            df = df.union(temp_df)\n",
    "\n",
    "# Assign numerical value to each class\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "# One hot encode a sparse vector representing the numerical class index\n",
    "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVector\")\n",
    "encoded = encoder.transform(indexed)\n",
    "\n",
    "# Creates an vector object representing input columns to be passed into an ml algorithm\n",
    "vectorAssembler = VectorAssembler(inputCols=['x','y','z'], outputCol=\"features\")\n",
    "features_vectorized = vectorAssembler.transform(encoded)\n",
    "\n",
    "# Normalize features\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "normalized = normalizer.transform(features_vectorized)\n",
    "\n",
    "# Create a pipeline with the desired data processing stages\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\n",
    "model = pipeline.fit(df)\n",
    "prediction = model.transform(df)\n",
    "\n",
    "# Visualize the transformations\n",
    "prediction.show()\n",
    "\n",
    "# Drop unnecessary columns, leaving only the processed features column and the vectorized category column\n",
    "df_train = prediction.drop('x').drop('y').drop('z').drop(\"class\").drop(\"source\").drop(\"features\").drop(\"classIndex\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System ML\n",
    "\n",
    "System ML enables algorithms to be reused across data-parallel frameworks such as Hadoop and Spark, streamlining the deployment process in varying environments. It provides an API called MLContext that allows the user to register RDDs and Dataframes that were previously created through Spark SQL or other libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning with Spark ML\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "First, create a Vector Assembler and Normalizer. Then create a Linear Regression model. Finally, combine stages into a Pipeline.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, normalizer, lr])\n",
    "model = pipeline.fit(df)\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# r2 value\n",
    "print(model.stages[2].summary.r2)\n",
    "```\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "Logistic regression is simply linear regression that has been passed into a sigmoid function. It is a supervised machine learning algorithm used to predict discrete categorical values.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.regression import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities & Naive Bayes\n",
    "\n",
    "* Marginal probability - independent of any other event\n",
    "* Joint probability - probability of events occuring together\n",
    "* Conditional probability - probability of an event given that another event has occurred\n",
    "\n",
    "**Bayes Rule Derivation**\n",
    "\n",
    "* Sum Rule: $P(x) = \\sum_{y}P(x,y)$\n",
    "* Product Rule: $P(x,y) = P(y|x)p(x)$\n",
    "\n",
    "Rearranging the product rule, we can derive the Bayes rule:\n",
    "\n",
    "$P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$\n",
    "\n",
    "This enables us to describe the probability of an event occuring based on prior knowledge of other events.\n",
    "\n",
    "**Gaussian Distribution**\n",
    "\n",
    "The Gaussian (or Normal) distribution is a very common continuous distribution that occurs naturally in nature. Because it is a valid probability density function, the area under the curve always sums to one. The Guassian is often used in machine learning because it is a byproduct of sampling any random distribution with finite variance. However, Bayes can also utilize different distributions, including Binomial and Multinomial.\n",
    "\n",
    "$N(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n",
    "\n",
    "Bayesian inference is the process of adjusting the probability of a hypothesis as new evidence becomes available. This involves:\n",
    "\n",
    "* Obtaining a prior hypothesis (distribution) $P(H)$\n",
    "* Collecting of new data $E$ with a marginal likelihood $P(E)$\n",
    "* Calculating the likelihood, i.e. how compatible the new data is to our prior knowledge of existing data $P(E|H)$\n",
    "* Obtaining a posterior, i.e. the probability of our hypothesis $P(H|E) = \\frac{P(E|H)*P(H)}{P(E)}$\n",
    "\n",
    "The likelihood is calculated by plugging the new data into a guassian equation, which is defined by the $\\mu$ and $\\sigma$ of the original data.\n",
    "\n",
    "The goal is to maximize the posterior distribution, i.e. select the $H$ which maximizes $\\frac{P(E|H)*P(H)}{P(E)}$. Notice that the denominator can be ignored, leaving only the numerator. This is called the maximum a posteriori aka MAP.\n",
    "\n",
    "Naive Bayes is \"naive\" because it assumes that, when $x$ is a vector with multiple features, that all features are conditionally independent. This enables us to make a simplification in our calculations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
